{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# üåå SSZ Full Pipeline - Google Colab\n",
    "\n",
    "**Segmented Spacetime Mass Projection - Complete Analysis Pipeline**\n",
    "\n",
    "¬© 2025 Carmen Wrede, Lino Casu  \n",
    "Licensed under the ANTI-CAPITALIST SOFTWARE LICENSE v1.4\n",
    "\n",
    "---\n",
    "\n",
    "## üìã What does this notebook do?\n",
    "\n",
    "- ‚úÖ Automatically installs all dependencies\n",
    "- ‚úÖ Clones the GitHub repository with Git LFS (large files included)\n",
    "- ‚úÖ Runs the complete SSZ pipeline with all 69 tests\n",
    "- ‚úÖ Generates all reports and plots\n",
    "- ‚úÖ Optional: Segment-Redshift Add-on\n",
    "- ‚úÖ Downloadable results\n",
    "\n",
    "**‚è±Ô∏è Runtime:** ~20-30 minutes (includes 3.6 GB download)\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Quick Start\n",
    "\n",
    "**Simply run all cells in sequence:**\n",
    "- `Runtime` ‚Üí `Run all` (Ctrl+F9)\n",
    "- Or individually: ‚ñ∂Ô∏è button for each cell\n",
    "\n",
    "**That's it!** All large files are downloaded automatically.\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Configuration (Optional)\n",
    "\n",
    "By default, the notebook downloads all large files (~3.6 GB).\n",
    "\n",
    "**To run faster with small files only:**\n",
    "- Go to Configuration cell below\n",
    "- Change `USE_GIT_LFS = True` to `USE_GIT_LFS = False`\n",
    "- Runtime: ~5-10 minutes instead of ~20-30 minutes\n",
    "- Tests will use v1/nightly datasets (limited but functional)\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Documentation\n",
    "\n",
    "For more details, see:\n",
    "- **GOOGLE_COLAB_SETUP.md** - Complete setup guide\n",
    "- **README_CLONE_TEST.md** - Clone and test instructions\n",
    "- **GIT_HYBRID_STRATEGY.md** - Technical details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config"
   },
   "source": [
    "## ‚öôÔ∏è Configuration\n",
    "\n",
    "### Repository Settings\n",
    "REPO_URL = \"https://github.com/error-wtf/Segmented-Spacetime-Mass-Projection-Unified-Results\"\n",
    "REPO_NAME = \"Segmented-Spacetime-Mass-Projection-Unified-Results\"\n",
    "\n",
    "### Git LFS Settings (for large files)\n",
    "USE_GIT_LFS = True  # Default: Download large files automatically (~3.6 GB, +15 min)\n",
    "                    # Set to False for small files only (~36 MB, faster but limited tests)\n",
    "\n",
    "### Pipeline Settings\n",
    "ENABLE_EXTENDED_METRICS = True   # Extended plots and statistics\n",
    "ENABLE_SEGMENT_REDSHIFT = True   # Gravitational redshift analysis\n",
    "\n",
    "print(\"‚úÖ Configuration loaded\")\n",
    "print(f\"üì¶ Repository: {REPO_NAME}\")\n",
    "print(f\"‚ö° Git LFS: {'Enabled (large files)' if USE_GIT_LFS else 'Disabled (small files only)'}\")\n",
    "print(f\"üìä Extended Metrics: {ENABLE_EXTENDED_METRICS}\")\n",
    "print(f\"üåå Segment Redshift: {ENABLE_SEGMENT_REDSHIFT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "config-cell"
   },
   "outputs": [],
   "source": [
    "%%capture install_output\n",
    "# Installation (output is captured to keep terminal clean)\n",
    "\n",
    "# Core scientific + astronomy\n",
    "!pip install -q numpy scipy pandas matplotlib astropy astroquery\n",
    "\n",
    "# Testing framework\n",
    "!pip install -q pytest pytest-timeout\n",
    "\n",
    "# Data formats\n",
    "!pip install -q pyarrow pyyaml\n",
    "\n",
    "# Utils\n",
    "!pip install -q requests tqdm colorama\n",
    "\n",
    "print(\"‚úÖ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install"
   },
   "source": [
    "# Show summary\n",
    "print(\"üì¶ Installed Packages:\")\n",
    "!pip list | grep -E \"numpy|scipy|pandas|matplotlib|astropy|astroquery|pytest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-deps"
   },
   "outputs": [],
   "source": [
    "%%capture install_output\n",
    "# Installation (output is captured to keep terminal clean)\n",
    "\n",
    "!pip install -q numpy scipy pandas matplotlib astropy requests tqdm\n",
    "\n",
    "print(\"‚úÖ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "show-install-summary"
   },
   "outputs": [],
   "source": [
    "# Show summary\n",
    "print(\"üì¶ Installed Packages:\")\n",
    "!pip list | grep -E \"numpy|scipy|pandas|matplotlib|astropy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clone"
   },
   "source": [
    "## üì• 2. Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone-repo"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION (with fallback defaults)\n",
    "# ============================================================================\n",
    "# If you haven't run the configuration cell above, use these defaults:\n",
    "try:\n",
    "    REPO_URL\n",
    "except NameError:\n",
    "    REPO_URL = \"https://github.com/error-wtf/Segmented-Spacetime-Mass-Projection-Unified-Results\"\n",
    "    REPO_NAME = \"Segmented-Spacetime-Mass-Projection-Unified-Results\"\n",
    "    USE_GIT_LFS = True  # Default: large files automatically\n",
    "    print(\"‚ö†Ô∏è  Using default configuration (large files enabled)\")\n",
    "    print(\"üí° To customize, run the Configuration cell first!\\n\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üì• REPOSITORY SETUP\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Repository: {REPO_NAME}\")\n",
    "print(f\"Git LFS: {'Enabled' if USE_GIT_LFS else 'Disabled (small files only)'}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Install Git LFS if requested\n",
    "if USE_GIT_LFS:\n",
    "    print(\"\\nüì¶ Installing Git LFS...\")\n",
    "    !apt-get install -y git-lfs > /dev/null 2>&1\n",
    "    !git lfs install --skip-smudge\n",
    "    print(\"‚úÖ Git LFS installed\")\n",
    "\n",
    "# Check if repository already exists\n",
    "if Path(REPO_NAME).exists():\n",
    "    print(f\"\\n‚ö†Ô∏è  Repository already exists: {REPO_NAME}\")\n",
    "    print(\"üîÑ Pulling latest changes...\")\n",
    "    !cd {REPO_NAME} && git pull\n",
    "    \n",
    "    # Pull LFS files if enabled\n",
    "    if USE_GIT_LFS:\n",
    "        print(\"‚¨áÔ∏è  Updating LFS files...\")\n",
    "        !cd {REPO_NAME} && git lfs pull\n",
    "else:\n",
    "    # Clone repository (skip LFS smudge to avoid filter errors)\n",
    "    print(f\"\\nüì• Cloning repository...\")\n",
    "    print(f\"   URL: {REPO_URL}\")\n",
    "    print(f\"   Strategy: {'Git LFS (large files)' if USE_GIT_LFS else 'Small files only'}\")\n",
    "    \n",
    "    if USE_GIT_LFS:\n",
    "        # Clone with skip-smudge to avoid checkout errors\n",
    "        os.environ['GIT_LFS_SKIP_SMUDGE'] = '1'\n",
    "    \n",
    "    !git clone --depth 1 {REPO_URL} {REPO_NAME}\n",
    "    \n",
    "    # Pull large files AFTER clone if LFS is enabled\n",
    "    if USE_GIT_LFS:\n",
    "        print(\"\\n‚¨áÔ∏è  Downloading large files (~3.6 GB, this may take 10-15 minutes)...\")\n",
    "        print(\"   Using git lfs pull (avoids smudge filter errors)...\")\n",
    "        !cd {REPO_NAME} && git lfs pull\n",
    "        print(\"‚úÖ Large files downloaded\")\n",
    "    else:\n",
    "        print(\"\\n‚ö° Using small files only (~36 MB)\")\n",
    "        print(\"   Tests with v1/nightly datasets will work immediately!\")\n",
    "        print(\"   üí° To get large files later, set USE_GIT_LFS=True in config and re-run\")\n",
    "\n",
    "# Change to repository directory\n",
    "os.chdir(REPO_NAME)\n",
    "print(f\"\\n‚úÖ Repository ready!\")\n",
    "print(f\"üìÇ Working Directory: {os.getcwd()}\")\n",
    "\n",
    "# Show what's available\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìÑ AVAILABLE FILES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check small test file\n",
    "small_test = Path(\"models/cosmology/2025-10-17_gaia_ssz_v1/ssz_field.parquet\")\n",
    "if small_test.exists():\n",
    "    size_mb = small_test.stat().st_size / (1024 * 1024)\n",
    "    print(f\"‚úÖ Small files: {size_mb:.2f} MB (v1/nightly datasets)\")\n",
    "else:\n",
    "    print(\"‚ùå Small files missing!\")\n",
    "\n",
    "# Check large test file\n",
    "large_test = Path(\"models/cosmology/2025-10-17_gaia_ssz_real/ssz_field.parquet\")\n",
    "if large_test.exists():\n",
    "    size_mb = large_test.stat().st_size / (1024 * 1024)\n",
    "    if size_mb > 100:\n",
    "        print(f\"‚úÖ Large files: {size_mb:.2f} MB (real-data complete)\")\n",
    "    else:\n",
    "        print(f\"‚ö° Large files: {size_mb*1024:.2f} KB (LFS pointers only)\")\n",
    "        print(\"   ‚ö†Ô∏è  Large files not downloaded - check git lfs pull output above\")\n",
    "else:\n",
    "    print(\"‚ùå Large files missing!\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "verify"
   },
   "source": [
    "## üîç 3. Verify Repository Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "verify-structure"
   },
   "outputs": [],
   "source": [
    "# Check required files\n",
    "required_files = [\n",
    "    \"run_full_suite.py\",\n",
    "    \"run_all_ssz_terminal.py\",\n",
    "    \"data/real_data_full.csv\",\n",
    "    \"scripts/addons/segment_redshift_addon.py\",\n",
    "    \"tests/test_ring_datasets.py\"\n",
    "]\n",
    "\n",
    "print(\"üîç Checking repository structure...\\n\")\n",
    "all_ok = True\n",
    "for file in required_files:\n",
    "    exists = Path(file).exists()\n",
    "    icon = \"‚úÖ\" if exists else \"‚ùå\"\n",
    "    print(f\"{icon} {file}\")\n",
    "    if not exists:\n",
    "        all_ok = False\n",
    "\n",
    "if all_ok:\n",
    "    print(\"\\n‚úÖ All required files present!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Some files missing - pipeline may run with limitations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "env"
   },
   "source": [
    "## üåç 4. Set Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set-env"
   },
   "outputs": [],
   "source": [
    "# Fallback defaults if configuration not run\n",
    "try:\n",
    "    ENABLE_EXTENDED_METRICS\n",
    "except NameError:\n",
    "    ENABLE_EXTENDED_METRICS = True\n",
    "    ENABLE_SEGMENT_REDSHIFT = True\n",
    "    print(\"‚ö†Ô∏è  Using default pipeline settings\")\n",
    "    print(\"üí° To customize, run the Configuration cell first!\\n\")\n",
    "\n",
    "# UTF-8 Encoding for cross-platform compatibility\n",
    "os.environ['PYTHONIOENCODING'] = 'utf-8:replace'\n",
    "os.environ['LANG'] = 'en_US.UTF-8'\n",
    "\n",
    "# Pipeline Features\n",
    "if ENABLE_EXTENDED_METRICS:\n",
    "    os.environ['SSZ_EXTENDED_METRICS'] = '1'\n",
    "    print(\"‚úÖ Extended Metrics enabled\")\n",
    "else:\n",
    "    os.environ['SSZ_EXTENDED_METRICS'] = '0'\n",
    "    print(\"‚è≠Ô∏è  Extended Metrics disabled\")\n",
    "\n",
    "if ENABLE_SEGMENT_REDSHIFT:\n",
    "    os.environ['SSZ_SEGMENT_REDSHIFT'] = '1'\n",
    "    print(\"‚úÖ Segment-Redshift Add-on enabled\")\n",
    "else:\n",
    "    os.environ['SSZ_SEGMENT_REDSHIFT'] = '0'\n",
    "    print(\"‚è≠Ô∏è  Segment-Redshift Add-on disabled\")\n",
    "\n",
    "print(\"\\nüåç Environment configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run"
   },
   "source": [
    "## üöÄ 5. Run Full Test Suite & Pipeline\n",
    "\n",
    "**This is the main execution - takes ~3-5 minutes!**\n",
    "\n",
    "The complete test suite executes:\n",
    "1. **Phase 1:** Root-level tests (6 physics tests)\n",
    "2. **Phase 2:** SegWave tests (20 tests)\n",
    "3. **Phase 3:** Multi-Ring validation tests (11 tests) ‚≠ê NEW!\n",
    "4. **Phase 4:** Scripts tests (5 tests)\n",
    "5. **Phase 5:** Cosmos tests (1 test)\n",
    "6. **Phase 6:** Complete SSZ Analysis (run_all_ssz_terminal.py - includes all pytest)\n",
    "7. **Phase 7:** SSZ Theory Predictions (4 tests)\n",
    "8. **Phase 8:** Example runs (G79, Cygnus X)\n",
    "9. **Phase 9:** Paper export tools\n",
    "\n",
    "**Output:** \n",
    "- `reports/RUN_SUMMARY.md` - Compact test overview\n",
    "- `reports/full-output.md` - Complete log (~230 KB)\n",
    "- All plots and analysis results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run-pipeline"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üöÄ SSZ FULL TEST SUITE & PIPELINE START\")\n",
    "print(\"=\"*80)\n",
    "print(f\"‚è∞ Start: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Run complete test suite (includes all tests + SSZ pipeline)\n",
    "print(\"üìã Running complete test suite...\")\n",
    "print(\"   This includes:\")\n",
    "print(\"   - Phase 1-5: All pytest tests (including 11 new ring validation tests)\")\n",
    "print(\"   - Phase 6: Complete SSZ Analysis (run_all_ssz_terminal.py)\")\n",
    "print(\"   - Phase 7-9: Theory predictions, examples, paper exports\")\n",
    "print()\n",
    "\n",
    "!python run_full_suite.py\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "minutes = int(elapsed // 60)\n",
    "seconds = int(elapsed % 60)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ TEST SUITE & PIPELINE COMPLETED\")\n",
    "print(\"=\"*80)\n",
    "print(f\"‚è±Ô∏è  Runtime: {minutes} min {seconds} sec\")\n",
    "print(f\"‚è∞ End: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "results"
   },
   "source": [
    "## üìä 6. Check Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check-results"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "print(\"üìä Generated Reports:\\n\")\n",
    "\n",
    "# Reports\n",
    "report_files = [\n",
    "    \"reports/full-output.md\",\n",
    "    \"reports/summary-output.md\",\n",
    "    \"reports/RUN_SUMMARY.md\",\n",
    "    \"reports/segment_redshift.csv\",\n",
    "    \"reports/segment_redshift.md\"\n",
    "]\n",
    "\n",
    "for file in report_files:\n",
    "    if Path(file).exists():\n",
    "        size = Path(file).stat().st_size / 1024  # KB\n",
    "        print(f\"‚úÖ {file:<45} ({size:.1f} KB)\")\n",
    "    else:\n",
    "        print(f\"‚è≠Ô∏è  {file:<45} (not generated)\")\n",
    "\n",
    "# Count plots\n",
    "print(\"\\nüìà Generated Plots:\\n\")\n",
    "plot_dirs = [\"reports/figures\", \"out\", \"agent_out/figures\", \"vfall_out\"]\n",
    "\n",
    "total_plots = 0\n",
    "for plot_dir in plot_dirs:\n",
    "    if Path(plot_dir).exists():\n",
    "        png_files = list(Path(plot_dir).rglob(\"*.png\"))\n",
    "        svg_files = list(Path(plot_dir).rglob(\"*.svg\"))\n",
    "        count = len(png_files) + len(svg_files)\n",
    "        total_plots += count\n",
    "        if count > 0:\n",
    "            print(f\"  {plot_dir:<30} {count} plots\")\n",
    "\n",
    "print(f\"\\nüìä **Total: {total_plots} plot files**\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-summary"
   },
   "source": [
    "## üìÑ 7. View Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "show-summary"
   },
   "outputs": [],
   "source": [
    "# Show RUN_SUMMARY.md\n",
    "summary_file = Path(\"reports/RUN_SUMMARY.md\")\n",
    "if summary_file.exists():\n",
    "    print(\"=\"*80)\n",
    "    print(\"üìÑ RUN SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(summary_file.read_text(encoding='utf-8'))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  RUN_SUMMARY.md not found\")\n",
    "\n",
    "# Segment-Redshift result\n",
    "seg_file = Path(\"reports/segment_redshift.md\")\n",
    "if seg_file.exists():\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üåå SEGMENT REDSHIFT RESULT\")\n",
    "    print(\"=\"*80)\n",
    "    print(seg_file.read_text(encoding='utf-8'))\n",
    "else:\n",
    "    print(\"\\n‚è≠Ô∏è  Segment-Redshift was not executed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "plots"
   },
   "source": [
    "## üñºÔ∏è 8. Show Example Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "show-plots"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image as PILImage\n",
    "\n",
    "# Search for interesting plots\n",
    "example_plots = [\n",
    "    \"reports/figures/fig_shared_segment_redshift_profile.png\",\n",
    "    \"out/phi_step_residual_hist.png\",\n",
    "    \"reports/figures/DemoObject/fig_DemoObject_ringchain_v_vs_k.png\"\n",
    "]\n",
    "\n",
    "print(\"üñºÔ∏è  Example Plots:\\n\")\n",
    "\n",
    "for plot_path in example_plots:\n",
    "    if Path(plot_path).exists():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üìä {plot_path}\")\n",
    "        print('='*60)\n",
    "        \n",
    "        # Display image\n",
    "        img = PILImage.open(plot_path)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"‚è≠Ô∏è  {plot_path} not found\")\n",
    "\n",
    "print(\"\\n‚úÖ More plots can be found in the reports/figures/ directories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download"
   },
   "source": [
    "## üíæ 9. Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zip-results"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "# Create ZIP archive\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "zip_name = f\"SSZ_Results_{timestamp}\"\n",
    "\n",
    "print(f\"üì¶ Creating ZIP archive: {zip_name}.zip\\n\")\n",
    "\n",
    "# Directories to pack\n",
    "dirs_to_zip = [\"reports\", \"out\", \"agent_out\"]\n",
    "\n",
    "# Temporary directory for archive\n",
    "temp_dir = Path(\"/tmp\") / zip_name\n",
    "temp_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Copy results\n",
    "for dir_name in dirs_to_zip:\n",
    "    src = Path(dir_name)\n",
    "    if src.exists():\n",
    "        dst = temp_dir / dir_name\n",
    "        shutil.copytree(src, dst, dirs_exist_ok=True)\n",
    "        print(f\"‚úÖ {dir_name} copied\")\n",
    "\n",
    "# Create ZIP\n",
    "shutil.make_archive(str(temp_dir), 'zip', temp_dir)\n",
    "zip_path = f\"{temp_dir}.zip\"\n",
    "\n",
    "size_mb = Path(zip_path).stat().st_size / (1024 * 1024)\n",
    "print(f\"\\n‚úÖ ZIP archive created: {zip_path}\")\n",
    "print(f\"üìä Size: {size_mb:.2f} MB\")\n",
    "\n",
    "# Download link (in Colab)\n",
    "try:\n",
    "    from google.colab import files\n",
    "    print(\"\\n‚¨áÔ∏è  Starting download...\")\n",
    "    files.download(zip_path)\n",
    "    print(\"‚úÖ Download started!\")\n",
    "except ImportError:\n",
    "    print(f\"\\nüí° Manual download from: {zip_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cleanup"
   },
   "source": [
    "## üßπ 10. Cleanup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cleanup-cell"
   },
   "outputs": [],
   "source": [
    "# Optional: Delete cache and temporary files\n",
    "import shutil\n",
    "\n",
    "print(\"üßπ Cleanup...\\n\")\n",
    "\n",
    "cache_dirs = [\n",
    "    \"__pycache__\",\n",
    "    \".pytest_cache\",\n",
    "    \"scripts/__pycache__\",\n",
    "    \"tests/__pycache__\"\n",
    "]\n",
    "\n",
    "for cache_dir in cache_dirs:\n",
    "    if Path(cache_dir).exists():\n",
    "        shutil.rmtree(cache_dir)\n",
    "        print(f\"‚úÖ {cache_dir} deleted\")\n",
    "\n",
    "print(\"\\n‚úÖ Cleanup completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "info"
   },
   "source": [
    "---\n",
    "\n",
    "## üìö More Information\n",
    "\n",
    "### üîó Links\n",
    "- **GitHub:** https://github.com/error-wtf/Segmented-Spacetime-Mass-Projection-Unified-Results\n",
    "- **License:** ANTI-CAPITALIST SOFTWARE LICENSE v1.4\n",
    "\n",
    "### üìñ Documentation\n",
    "- `README.md` - Project overview\n",
    "- `papers/` - Scientific papers\n",
    "- `reports/` - Generated analyses\n",
    "\n",
    "### üéØ Pipeline Features\n",
    "- **35 physics tests** with detailed interpretations\n",
    "- **23 technical tests** (silent mode)\n",
    "- **Extended metrics** - Additional plots and statistics\n",
    "- **Segment-Redshift add-on** - Gravitational redshift\n",
    "\n",
    "### ‚öôÔ∏è Customize Configuration\n",
    "Go back to the **Configuration cell** (above) and modify:\n",
    "```python\n",
    "ENABLE_EXTENDED_METRICS = True/False\n",
    "ENABLE_SEGMENT_REDSHIFT = True/False\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "¬© 2025 Carmen Wrede, Lino Casu\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "SSZ Full Pipeline",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
