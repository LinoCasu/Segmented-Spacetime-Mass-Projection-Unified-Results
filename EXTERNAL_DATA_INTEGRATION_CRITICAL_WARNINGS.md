# ‚ö†Ô∏è CRITICAL WARNINGS: External Data Integration

**Date:** 2025-10-19  
**Audience:** Researchers integrating external datasets  
**Priority:** üî¥ **CRITICAL - READ BEFORE INTEGRATING DATA**

---

## üö® **EXECUTIVE SUMMARY**

Integrating external datasets into the SSZ Suite requires **EXTREME CARE**. The pipeline consists of interconnected scripts that expect **specific column structures** and **data consistency**. Failure to properly validate and merge data can lead to:

- ‚ùå Silent failures (NaN propagation)
- ‚ùå Incorrect scientific results
- ‚ùå Test suite crashes
- ‚ùå Data inconsistencies across debug files

**GOLDEN RULE:** When in doubt, validate **three times**. Then validate again.

---

## üìä **THREE LEVELS OF INTEGRATION COMPLEXITY**

### **Level 1: Single Script Integration** üü¢ SAFE

**Scenario:** You want to run **ONE** test script with your own data.

**Risk Level:** üü¢ **LOW** - Individual scripts have isolated requirements

**Example:**
```bash
# Run single test with custom data
python scripts/tests/test_horizon_hawking_predictions.py --custom-data my_data.csv
```

**Requirements:**
- ‚úÖ Only needs columns required by THAT script
- ‚úÖ Script-specific validation (e.g., `z_obs` for Hawking test)
- ‚úÖ NaN gaps OK if script filters them out

**Validation Needed:** Basic (check script's column requirements)

---

### **Level 2: Partial Pipeline Integration** üü° MODERATE

**Scenario:** You want to run **MULTIPLE** related scripts (e.g., all prediction tests).

**Risk Level:** üü° **MODERATE** - Scripts share intermediate files

**Example:**
```bash
# Run test suite (multiple scripts)
python run_full_suite.py --custom-data my_data.csv
```

**Requirements:**
- ‚ö†Ô∏è Shared debug files (`out/phi_step_debug_full.csv`, `out/_enhanced_debug.csv`)
- ‚ö†Ô∏è Each script may expect different columns from same file
- ‚ö†Ô∏è NaN in one file can break downstream scripts

**Validation Needed:** Moderate (check all used scripts' requirements + consistency)

---

### **Level 3: Full Pipeline Integration** üî¥ CRITICAL

**Scenario:** You want to run the **COMPLETE SSZ SUITE** (`run_all_ssz_terminal.py`) with your data.

**Risk Level:** üî¥ **CRITICAL** - Multiple interdependent scripts, shared state

**Pipeline Flow:**
```
real_data_full.csv (input)
    ‚Üì
phi_test.py ‚Üí out/phi_step_debug_full.csv
    ‚Üì
segspace_enhanced_test_better_final.py ‚Üí out/_enhanced_debug.csv
    ‚Üì
test_horizon_hawking_predictions.py (reads both files!)
    ‚Üì
test_hawking_spectrum_continuum.py (reads both files!)
    ‚Üì
... (more scripts)
```

**CRITICAL ISSUES:**

1. **Column Propagation**
   - ‚ùå Missing column in `real_data_full.csv` ‚Üí Missing in ALL downstream files
   - ‚ùå NaN in source ‚Üí NaN propagates through entire pipeline
   - ‚ùå One script's filter ‚Üí Inconsistent row counts downstream

2. **Cross-Script Dependencies**
   - Script A generates `phi_step_debug_full.csv` with 24 columns
   - Script B reads it and expects column X
   - Script C reads it and expects column Y (‚â† X)
   - **IF X or Y missing:** Silent failure or crash

3. **Merge Hell**
   - Different scripts merge data from different sources
   - Manual merges can create:
     - ‚ùå Duplicate rows
     - ‚ùå Misaligned indices
     - ‚ùå Inconsistent source names
     - ‚ùå NaN gaps where data should exist

**Validation Needed:** üî¥ **EXTREME** (triple/quadruple checks mandatory)

---

## üîç **CRITICAL VALIDATION WORKFLOW**

### **Step 1: Pre-Integration Validation** 

**BEFORE adding ANY external data to `real_data_full.csv`:**

```bash
# 1. Validate structure
python scripts/data_generators/validate_dataset.py

# 2. Check column completeness
python check_column_completeness.py

# 3. Check critical columns
python -c "
import pandas as pd
df = pd.read_csv('real_data_full.csv')

CRITICAL = ['source', 'f_emit_Hz', 'f_obs_Hz', 'r_emit_m', 'M_solar', 'z']
for col in CRITICAL:
    nan_count = df[col].isna().sum()
    print(f'{col}: {nan_count} NaN / {len(df)} rows')
    if nan_count > 0:
        print(f'  ‚ö†Ô∏è WARNING: {col} has NaN gaps!')
"
```

**MANDATORY CHECKS:**

| Check | Tool | Failure Action |
|-------|------|----------------|
| All critical columns present | `validate_dataset.py` | ‚ùå STOP - Add missing columns |
| No NaN in critical columns | Manual check above | ‚ùå STOP - Fill or remove rows |
| Source names consistent | Visual inspection | ‚ùå STOP - Standardize names |
| Frequency values positive | `validate_dataset.py` | ‚ùå STOP - Fix or remove |
| Mass values reasonable | `validate_dataset.py` | ‚ö†Ô∏è WARN - Review |

---

### **Step 2: Integration with Validation**

**USE OUR INTEGRATION TOOL** (has built-in validation):

```bash
# ‚úÖ RECOMMENDED: Use our validator
python scripts/data_generators/integrate_ned_spectrum.py \
    --ned-file your_data.csv \
    --source-name "Your Source" \
    --mass-solar 1e6 \
    --validate  # ‚Üê CRITICAL FLAG
```

**What it checks:**
- ‚úÖ Column structure matches `real_data_full.csv`
- ‚úÖ No duplicate rows added
- ‚úÖ All critical columns filled (no NaN)
- ‚úÖ Frequency/mass values reasonable
- ‚úÖ Creates backup before merging

**‚ùå DO NOT manually append to CSV without validation!**

---

### **Step 3: Post-Integration Validation**

**AFTER adding data, verify EVERYTHING:**

```bash
# 1. Validate merged dataset
python scripts/data_generators/validate_dataset.py

# 2. Count rows (should increase correctly)
python -c "
import pandas as pd
df = pd.read_csv('real_data_full.csv')
print(f'Total rows: {len(df)}')
print(f'Unique sources: {df[\"source\"].nunique()}')
print(f'Expected: [YOUR CALCULATION]')
"

# 3. Check for duplicates
python -c "
import pandas as pd
df = pd.read_csv('real_data_full.csv')
dupes = df.duplicated(subset=['source', 'f_emit_Hz'], keep=False)
print(f'Duplicate rows: {dupes.sum()}')
if dupes.sum() > 0:
    print('‚ö†Ô∏è WARNING: Duplicates found!')
    print(df[dupes][['source', 'f_emit_Hz']])
"

# 4. Run pipeline to regenerate debug files
python run_all_ssz_terminal.py

# 5. Verify debug files consistency
python check_column_completeness.py
```

---

## üß© **NaN GAPS: WHEN ACCEPTABLE, WHEN FATAL**

### **‚úÖ ACCEPTABLE NaN** (Expected, Scientifically Valid)

| Column | Acceptable NaN | Reason |
|--------|---------------|---------|
| `a_m`, `e`, `P_year` | ‚úÖ YES | Continuum spectra have no orbits |
| `z_geom_hint`, `N0`, `n_star` | ‚úÖ YES | Optional analysis parameters |
| `lambda_emit_nm`, `lambda_obs_nm` | ‚úÖ YES | Redundant with frequency |
| `v_los_mps`, `v_tot_mps` | ‚úÖ YES | Stationary sources have no velocity |

**Example:** M87 NED continuum spectrum (139 rows) - ALL have NaN for orbital parameters. **This is CORRECT.**

---

### **‚ùå FATAL NaN** (Will Break Tests)

| Column | Fatal if NaN | Breaks |
|--------|-------------|---------|
| `source` | ‚ùå FATAL | All scripts - can't group data |
| `f_emit_Hz` | ‚ùå FATAL | All frequency-based tests |
| `f_obs_Hz` | ‚ùå FATAL | All redshift calculations |
| `r_emit_m` | ‚ùå FATAL | All horizon/radius tests |
| `M_solar` | ‚ùå FATAL | All Schwarzschild radius calculations |
| `z` | ‚ùå FATAL | Most cosmological tests |
| `z_obs` (in enhanced) | ‚ùå FATAL | Hawking radiation test (chi calculation) |

**If ANY of these have NaN:**
1. ‚ùå DO NOT proceed with integration
2. ‚ùå Either fill with scientific estimate OR remove rows
3. ‚ùå Document your filling method in `DATA_FILLING_METHODS.md`

---

## üî¨ **SCIENTIFIC DATA FILLING** (Last Resort Only!)

**‚ö†Ô∏è WARNING:** Filling missing data can introduce bias. **AVOID if possible.**

### **When Filling is Acceptable:**

1. **Redundant Calculations**
   ```python
   # z and z_obs are usually identical for most sources
   if df['z_obs'].isna().any():
       df['z_obs'] = df['z_obs'].fillna(df['z'])
       # Document: "Assumed z_obs ‚âà z for stationary sources"
   ```

2. **Conservative Estimates**
   ```python
   # Emission radius from simplified luminosity distance
   # ONLY if actual radius unknown
   df['r_emit_m'] = estimate_radius_from_luminosity(df)
   # Document: "r_emit estimated via L = 4œÄr¬≤F (simplified)"
   ```

3. **Standard Values**
   ```python
   # Eccentricity for circular orbits
   df['e'] = df['e'].fillna(0.0)
   # Document: "e=0 assumed for circular orbits (conservative)"
   ```

### **‚ö†Ô∏è MANDATORY when Filling:**

1. **Document EVERYTHING:**
   - Create `DATA_FILLING_LOG.md`
   - Record: What filled, Why, Method, Uncertainty
   - Example:
     ```markdown
     ## M87 NED Spectrum (2025-10-19)
     - Filled: r_emit_m (139 rows)
     - Method: Simplified luminosity distance (D_L ‚âà 16.7 Mpc)
     - Uncertainty: ¬±30% (no proper r measurement for continuum)
     - Citation: Akiyama et al. 2019 (M87 distance)
     ```

2. **Add Warning Flags:**
   ```python
   df['data_quality_flag'] = 'FILLED'  # Mark filled rows
   df['filling_method'] = 'luminosity_distance_estimate'
   ```

3. **Validate Scientifically:**
   - Compare filled values to literature
   - Check if results change significantly
   - Run sensitivity analysis

---

## üõ†Ô∏è **OUR CONSISTENCY CHECK SCRIPTS**

We've developed multiple validation layers to catch inconsistencies:

### **1. `scripts/data_generators/validate_dataset.py`**

**Purpose:** Pre-integration validation of `real_data_full.csv`

**Checks:**
- ‚úÖ All critical columns present (100% filled)
- ‚úÖ Frequency values positive and reasonable
- ‚úÖ Mass values within physical limits
- ‚úÖ No duplicate source+frequency combinations
- ‚úÖ Redshift values scientifically acceptable

**Usage:**
```bash
python scripts/data_generators/validate_dataset.py
# Output: PASSED / FAILED with detailed report
```

**Our Safeguards:**
- Checks 7 critical columns for 100% completeness
- Validates 4 blueshift sources are scientifically correct
- Reports percentage filled for optional columns
- Exits with error code if critical issues found

---

### **2. `check_column_completeness.py`**

**Purpose:** Cross-check consistency between pipeline files

**Checks:**
- ‚úÖ `real_data_full.csv` has all source columns
- ‚úÖ `out/phi_step_debug_full.csv` has phi-related columns
- ‚úÖ `out/_enhanced_debug.csv` has enhanced columns (z_obs, deltaM, etc.)
- ‚ö†Ô∏è Identifies missing columns needed by tests
- ‚ö†Ô∏è Reports NaN gaps in critical columns

**Usage:**
```bash
python check_column_completeness.py
# Output: Reports missing columns and NaN counts
```

**Our Safeguards:**
- Compares 3 pipeline files for consistency
- Lists columns that `_enhanced_debug.csv` has but `phi_step` doesn't
- Identifies which test needs which column
- Suggests merge strategy if columns missing

---

### **3. `check_data_availability.py`**

**Purpose:** Verify test-specific data requirements

**Checks:**
- ‚úÖ Multi-frequency sources (for Jacobian test) - Need 3+ freq
- ‚úÖ Near-horizon data (for kappa_seg test) - Need r < 3 r_s
- ‚úÖ Continuum spectra (for Hawking spectrum test)
- ‚úÖ Source-specific coverage (M87, Sgr A*, S2, etc.)

**Usage:**
```bash
python check_data_availability.py
# Output: Reports data coverage for each test requirement
```

**Our Safeguards:**
- Checks 5 multi-frequency sources exist
- Validates 181 near-horizon observations (r < 3 r_s)
- Reports frequency range (9+ orders of magnitude)
- Identifies which tests will have "Insufficient data" warnings

---

### **4. `scripts/data_generators/integrate_ned_spectrum.py`**

**Purpose:** Safe external data integration with validation

**Built-in Checks:**
- ‚úÖ Column structure match before merge
- ‚úÖ No duplicate rows added
- ‚úÖ Critical columns 100% filled in new data
- ‚úÖ Source name consistency
- ‚úÖ Creates timestamped backup before merge

**Usage:**
```bash
python scripts/data_generators/integrate_ned_spectrum.py \
    --ned-file data.csv \
    --source-name "Source" \
    --mass-solar 1e6 \
    --validate
```

**Our Safeguards:**
- Validates BEFORE writing to `real_data_full.csv`
- Shows preview of what will be added
- Backs up original file (`real_data_full.backup_YYYYMMDD_HHMMSS.csv`)
- Verifies row count increase matches expected
- Reports detailed statistics post-merge

---

## üîÑ **RECOMMENDED INTEGRATION WORKFLOW**

```
Step 1: Prepare External Data
  ‚Üì
  [Manual] Clean source data (remove duplicates, standardize names)
  ‚Üì
Step 2: Pre-Validation
  ‚Üì
  [Auto] python scripts/data_generators/validate_dataset.py
  ‚Üì
  ‚úÖ PASSED? ‚Üí Continue
  ‚ùå FAILED? ‚Üí Fix issues, repeat Step 2
  ‚Üì
Step 3: Integration
  ‚Üì
  [Auto] python scripts/data_generators/integrate_ned_spectrum.py --validate
  ‚Üì
  Review preview, confirm merge
  ‚Üì
Step 4: Post-Validation (CRITICAL!)
  ‚Üì
  [Auto] python scripts/data_generators/validate_dataset.py  # Re-validate merged data
  [Auto] python check_column_completeness.py                # Check all files
  [Auto] python check_data_availability.py                  # Check test coverage
  ‚Üì
  ‚úÖ ALL PASSED? ‚Üí Continue to Step 5
  ‚ùå ANY FAILED? ‚Üí Restore backup, debug, repeat Step 3
  ‚Üì
Step 5: Pipeline Regeneration
  ‚Üì
  [Auto] python run_all_ssz_terminal.py  # Regenerate ALL debug files
  ‚Üì
  Wait for completion (~10-15 min)
  ‚Üì
Step 6: Final Verification
  ‚Üì
  [Auto] python check_column_completeness.py  # Verify debug files updated
  [Manual] Check test outputs for new data
  ‚Üì
Step 7: Test Suite
  ‚Üì
  [Auto] python run_full_suite.py --quick
  ‚Üì
  ‚úÖ ALL TESTS PASSED? ‚Üí Integration complete! üéâ
  ‚ùå TESTS FAILED? ‚Üí Review errors, check logs, debug
```

---

## üö´ **COMMON MISTAKES** (DON'T DO THESE!)

### ‚ùå **Mistake 1: Manual CSV Append**

```python
# ‚ùå WRONG!
df_old = pd.read_csv('real_data_full.csv')
df_new = pd.read_csv('my_data.csv')
df_merged = pd.concat([df_old, df_new])
df_merged.to_csv('real_data_full.csv', index=False)  # NO VALIDATION!
```

**Why bad:** No validation, no duplicate check, no backup, no column match verification

**‚úÖ CORRECT:** Use `integrate_ned_spectrum.py` with `--validate` flag

---

### ‚ùå **Mistake 2: Ignoring NaN in Critical Columns**

```python
# ‚ùå WRONG!
df = pd.read_csv('real_data_full.csv')
# "Oh, z_obs has 50 NaN? I'll just keep going..."
# ‚Üí CRASH in test_horizon_hawking_predictions.py later!
```

**‚úÖ CORRECT:** Fix or fill NaN BEFORE merging, document method

---

### ‚ùå **Mistake 3: Not Regenerating Debug Files**

```python
# ‚ùå WRONG!
# Add data to real_data_full.csv
# Run test_horizon_hawking_predictions.py directly
# ‚Üí Uses OLD phi_step_debug_full.csv (143 rows)
# ‚Üí New data (427 rows) not in debug files!
# ‚Üí Test says "Insufficient data" even though data exists
```

**‚úÖ CORRECT:** ALWAYS run `python run_all_ssz_terminal.py` after data changes

---

### ‚ùå **Mistake 4: Assuming All Scripts Use Same File**

```python
# ‚ùå WRONG ASSUMPTION!
# "I updated real_data_full.csv, so ALL tests will use it"
# 
# Reality:
# - Some tests read real_data_full.csv
# - Some tests read out/phi_step_debug_full.csv
# - Some tests read out/_enhanced_debug.csv
# ‚Üí Inconsistent data across tests!
```

**‚úÖ CORRECT:** Regenerate ALL debug files after merging

---

## üìù **INTEGRATION CHECKLIST**

Before integrating external data, check ALL boxes:

### Pre-Integration
- [ ] External data has ALL critical columns (source, f_emit_Hz, f_obs_Hz, r_emit_m, M_solar, z)
- [ ] Source names standardized (no "M87" vs "m87" vs "M 87")
- [ ] No duplicate source+frequency combinations
- [ ] Frequency values positive and reasonable (> 1e6 Hz, < 1e20 Hz)
- [ ] Mass values reasonable (> 1e-10 M_sun, < 1e12 M_sun)
- [ ] Created backup of `real_data_full.csv`

### Integration
- [ ] Used `integrate_ned_spectrum.py` with `--validate` flag
- [ ] Reviewed integration preview before confirming
- [ ] Integration reported success (no errors)
- [ ] Row count increased by expected amount

### Post-Integration
- [ ] Ran `validate_dataset.py` - PASSED
- [ ] Ran `check_column_completeness.py` - No missing critical columns
- [ ] Ran `check_data_availability.py` - Data coverage acceptable
- [ ] Checked for duplicates (zero found)
- [ ] Verified no NaN in critical columns

### Pipeline Regeneration
- [ ] Ran `python run_all_ssz_terminal.py` - Completed successfully
- [ ] Checked `out/phi_step_debug_full.csv` row count (should match `real_data_full.csv`)
- [ ] Checked `out/_enhanced_debug.csv` has `z_obs` column (0 NaN)
- [ ] Re-ran `check_column_completeness.py` - All debug files consistent

### Final Verification
- [ ] Ran `python run_full_suite.py --quick` - All tests passed
- [ ] Reviewed test outputs for warnings (acceptable vs critical)
- [ ] Documented integration in `NED_SPECTRUM_INTEGRATION_YYYY-MM-DD.md`
- [ ] If filled any data: Created `DATA_FILLING_LOG.md` with methods

### Documentation
- [ ] Updated `README.md` with new row count
- [ ] Updated `DATA_COLUMNS_README.md` if column structure changed
- [ ] Created integration summary document
- [ ] Committed changes with descriptive message

---

## üéØ **SUMMARY: THREE GOLDEN RULES**

1. **VALIDATE THREE TIMES**
   - Before integration
   - After integration
   - After pipeline regeneration

2. **NEVER SKIP DEBUG FILE REGENERATION**
   - Always run `python run_all_ssz_terminal.py` after data changes
   - Always verify debug files with `check_column_completeness.py`

3. **DOCUMENT EVERYTHING**
   - What data added
   - Where it came from
   - How it was processed
   - What was filled (if anything)
   - Why it's scientifically valid

---

## üìû **CONTACT & SUPPORT**

**If you encounter integration issues:**

1. **Check validation output:**
   ```bash
   python scripts/data_generators/validate_dataset.py 2>&1 | tee validation.log
   ```

2. **Review our integration examples:**
   - `NED_SPECTRUM_INTEGRATION_2025-10-19.md` - Full NED integration
   - `EXTERNAL_DATASETS_GUIDE.md` - General guidelines

3. **Common error solutions:**
   - "Column X missing" ‚Üí Add to source data before merging
   - "NaN in critical column" ‚Üí Fill or remove rows
   - "Test says insufficient data" ‚Üí Regenerate debug files
   - "Duplicate rows found" ‚Üí Check source name consistency

4. **Open issue on GitHub:**
   - Include validation output
   - Include `check_column_completeness.py` output
   - Include sample of your data (first 10 rows)

---

## ‚ö†Ô∏è **FINAL WARNING**

**Data integration for full pipeline runs is HARD.** 

If you're unsure, **start with Level 1** (single script) and work your way up. Don't attempt full pipeline integration on your first try.

**When in doubt:** Validate three times. Then validate again. üéØ

---

¬© 2025 Carmen Wrede, Lino Casu  
Licensed under the ANTI-CAPITALIST SOFTWARE LICENSE v1.4
